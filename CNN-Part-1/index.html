<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="IntroductionCNNs are the go-to neural network for images The fundamental difference between a densely connected layer and a convolutionlayer is this: Dense layers learn global patterns in their input">
<meta property="og:type" content="article">
<meta property="og:title" content="CNN : Watching the world through Neural Networks - Part 1">
<meta property="og:url" content="https://zarif98sjs.github.io/mindcraft/CNN-Part-1/index.html">
<meta property="og:site_name" content="MindCraft">
<meta property="og:description" content="IntroductionCNNs are the go-to neural network for images The fundamental difference between a densely connected layer and a convolutionlayer is this: Dense layers learn global patterns in their input">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zarif98sjs.github.io/mindcraft/CNN-Part-1/cnn_p1_img_1.png">
<meta property="og:image" content="https://zarif98sjs.github.io/mindcraft/CNN-Part-1/cnn_p1_img_2.png">
<meta property="og:image" content="https://zarif98sjs.github.io/mindcraft/CNN-Part-1/cnn_p1_img_3.png">
<meta property="og:image" content="https://zarif98sjs.github.io/mindcraft/CNN-Part-1/output_40_0.png">
<meta property="og:image" content="https://zarif98sjs.github.io/mindcraft/CNN-Part-1/output_40_1.png">
<meta property="article:published_time" content="2020-06-29T00:00:00.000Z">
<meta property="article:modified_time" content="2021-04-27T07:24:44.084Z">
<meta property="article:author" content="Md. Zarif Ul Alam">
<meta property="article:tag" content="Algo, DS, Software &amp; What Not!">
<meta property="article:tag" content="Learning Notes">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zarif98sjs.github.io/mindcraft/CNN-Part-1/cnn_p1_img_1.png">
    
    
      
        
          <link rel="shortcut icon" href="/mindcraft/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/mindcraft/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/mindcraft/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>CNN : Watching the world through Neural Networks - Part 1</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/mindcraft/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
			tex2jax: {
			  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			  inlineMath: [['$','$']]
			}
		  });
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
	
<meta name="generator" content="Hexo 5.4.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" "Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/mindcraft/">Home</a></li>
         
          <li><a href="/mindcraft/archives/">Archive</a></li>
         
          <li><a href="https://zarif98sjs.github.io/">About Me</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post " href="/mindcraft/CNN-Part-2/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post " href="/mindcraft/Regression/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top " href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post " href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://zarif98sjs.github.io/mindcraft/CNN-Part-1/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-1/&text=CNN : Watching the world through Neural Networks - Part 1"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-1/&title=CNN : Watching the world through Neural Networks - Part 1"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-1/&is_video=false&description=CNN : Watching the world through Neural Networks - Part 1"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=CNN : Watching the world through Neural Networks - Part 1&body=Check out this article: https://zarif98sjs.github.io/mindcraft/CNN-Part-1/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-1/&title=CNN : Watching the world through Neural Networks - Part 1"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-1/&title=CNN : Watching the world through Neural Networks - Part 1"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-1/&title=CNN : Watching the world through Neural Networks - Part 1"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-1/&title=CNN : Watching the world through Neural Networks - Part 1"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-1/&name=CNN : Watching the world through Neural Networks - Part 1&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://zarif98sjs.github.io/mindcraft/CNN-Part-1/&t=CNN : Watching the world through Neural Networks - Part 1"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Properties"><span class="toc-number">2.</span> <span class="toc-text">Properties</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Feature-Maps"><span class="toc-number">3.</span> <span class="toc-text">Feature Maps</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Filters"><span class="toc-number">4.</span> <span class="toc-text">Filters</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Parameters"><span class="toc-number">5.</span> <span class="toc-text">Parameters</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Convolution-Mechanism"><span class="toc-number">6.</span> <span class="toc-text">Convolution Mechanism</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Max-Pooling"><span class="toc-number">7.</span> <span class="toc-text">Max Pooling</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Training-CNN-from-scratch-on-a-small-dataset"><span class="toc-number">8.</span> <span class="toc-text">Training CNN from scratch on a small dataset</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Dataset"><span class="toc-number">9.</span> <span class="toc-text">Dataset</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Extracting-zip-files"><span class="toc-number">10.</span> <span class="toc-text">Extracting zip files</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Importing-Dependencies"><span class="toc-number">11.</span> <span class="toc-text">Importing Dependencies</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Preparing-Data"><span class="toc-number">12.</span> <span class="toc-text">Preparing Data</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Train-Validation-Split"><span class="toc-number">13.</span> <span class="toc-text">Train Validation Split</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Building-the-CNN"><span class="toc-number">14.</span> <span class="toc-text">Building the CNN</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Data-preprocessing"><span class="toc-number">15.</span> <span class="toc-text">Data preprocessing</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Train-Fit"><span class="toc-number">16.</span> <span class="toc-text">Train &#x2F; Fit</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Predict"><span class="toc-number">17.</span> <span class="toc-text">Predict</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Creating-Submission-File"><span class="toc-number">18.</span> <span class="toc-text">Creating Submission File</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Data-Augmentation"><span class="toc-number">19.</span> <span class="toc-text">Data Augmentation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Note"><span class="toc-number">19.0.1.</span> <span class="toc-text">Note</span></a></li></ol></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        CNN : Watching the world through Neural Networks - Part 1
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Md. Zarif Ul Alam</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2020-06-29T00:00:00.000Z" itemprop="datePublished">2020-06-29</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/mindcraft/categories/Machine-Learning/">Machine Learning</a> › <a class="category-link" href="/mindcraft/categories/Machine-Learning/Tensorflow/">Tensorflow</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/mindcraft/tags/Algo-DS-Software-What-Not/" rel="tag">Algo, DS, Software & What Not!</a>, <a class="tag-link-link" href="/mindcraft/tags/Learning-Notes/" rel="tag">Learning Notes</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h1 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h1><p><strong>CNNs are the go-to neural network for images</strong></p>
<p>The fundamental difference between a densely connected layer and a convolution<br>layer is this: Dense layers learn global patterns in their input feature space (for example, for a MNIST digit, patterns involving all pixels), whereas convolution layers learn<br>local patterns</p>
<img src="cnn_p1_img_1.png" />

<h1 id="Properties"><a href="#Properties" class="headerlink" title="Properties"></a>Properties</h1><p><strong>This key characteristic gives convnets two interesting <em>properties</em>:</strong></p>
<ul>
<li>The patterns they learn are <strong>translation invariant</strong> : After learning a certain pattern in the lower-right corner of a picture, a convnet can recognize it anywhere . A densely connected network would have to learn the pattern anew if it appeared at a new location . This makes convnets data efficient when processing images </li>
<li>They can learn spatial <strong>hierarchies of patterns</strong> : A first convolution layer will learn small local patterns such as <em>edges</em>, a second convolution layer will learn larger patterns made of the features of the first layers, and so on. This allows convnets to efficiently learn <em>increasingly complex and abstract visual concepts</em> (because the visual world is fundamentally spatially hierarchical)</li>
</ul>
<img src="cnn_p1_img_2.png" />

<h1 id="Feature-Maps"><a href="#Feature-Maps" class="headerlink" title="Feature Maps"></a>Feature Maps</h1><p>Convolutions operate over 3D tensors, called <em>feature maps</em>, with two spatial axes (height and width) as well as a depth axis (also called the channels axis). For an RGB image, the dimension of the depth axis is 3, because the image has three color channels: red, green, and blue. For a black-and-white picture, like the MNIST digits,the depth is 1 (levels of gray).</p>
<p>The convolution operation extracts patches from its input feature map and applies the same transformation to all of these patches, producing an output feature map. This output feature map is still a 3D tensor: it has a width and a height. <em>Its depth can be arbitrary, because the output depth is a parameter of the layer</em> , and the different channels in that depth axis no longer stand for specific colors as in RGB input; rather, they stand for <em>filters</em> . The 2D tensor output[:, :, n] is the 2D spatial map of the response of this filter over the input</p>
<h1 id="Filters"><a href="#Filters" class="headerlink" title="Filters"></a>Filters</h1><p>Filters encode specific aspects of the input data: at a high level, a single filter could encode the concept “presence of a face in the input,” for instance.</p>
<h1 id="Parameters"><a href="#Parameters" class="headerlink" title="Parameters"></a>Parameters</h1><p><strong>Convolutions are defined by two key parameters:</strong></p>
<ul>
<li>Size of the patches extracted from the inputs —These are typically <code>3 × 3</code> or <code>5 × 5</code></li>
<li>Depth of the output feature map—The number of filters computed by the convolution</li>
</ul>
<p><strong>In Keras Conv2D layers, these parameters are the first arguments passed to the layer: <code>Conv2D(output_depth, (window_height, window_width))</code></strong></p>
<h1 id="Convolution-Mechanism"><a href="#Convolution-Mechanism" class="headerlink" title="Convolution Mechanism"></a>Convolution Mechanism</h1><p><strong>A convolution works by sliding these windows of size <code>3 × 3</code> or <code>5 × 5</code> over the 3D input feature map, stopping at every possible location, and extracting the 3D patch of surrounding features (shape <code>(window_height, window_width, input_depth)</code>). Each such 3D patch is then transformed (via a tensor product with the same learned weight<br>matrix, called the <em>convolution kernel</em>) into a 1D vector of shape <code>(output_depth,)</code>.</strong></p>
<p><strong>All of these vectors are then spatially reassembled into a 3D output map of shape <code>(height,width, output_depth)</code>. Every spatial location in the output feature map corresponds to the same location in the input feature map</strong></p>
<img src="cnn_p1_img_3.png" />

<p>Note that the output width and height may differ from the input width and height . They may differ for two reasons:</p>
<ul>
<li><strong><code>Border effects</code></strong><ul>
<li>If you want to get an output feature map with the same spatial dimensions as the input, you can use padding . Padding consists of adding an appropriate number of rows and columns on each side of the input feature map so as to make it possible to fit center convolution windows around every input tile</li>
<li>In Conv2D layers, padding is configurable via the padding argument, which takes two values: <code>valid</code>, which means <strong>no padding</strong> (only valid window locations will be used); and <code>same</code>, which means “**pad in such a way as to have an output with the same width and height as the input.**” The padding argument defaults to <code>valid</code></li>
</ul>
</li>
<li><strong><code>Stride</code></strong><ul>
<li>The description of convolution so far has assumed that the center tiles of the convolution windows are all <em>contiguous</em>. But the distance between two successive windows is a parameter of the convolution, called its stride, which defaults to <code>1</code>.</li>
<li>Using stride <code>2</code> means the width and height of the feature map are <strong>downsampled</strong> by a factor of <code>2</code> (in addition to any changes induced by border effects). Strided convolutions are <strong>rarely used in practice</strong>, although they can come in handy for some types of models; it’s good to be familiar with the concept</li>
<li>To downsample feature maps, instead of strides, we tend to use the <code>max-pooling operation</code></li>
</ul>
</li>
</ul>
<h1 id="Max-Pooling"><a href="#Max-Pooling" class="headerlink" title="Max Pooling"></a>Max Pooling</h1><p>The role of max pooling: to aggressively downsample feature maps, much like strided convolutions</p>
<p>Max pooling consists of extracting windows from the input feature maps and outputting the max value of each channel. It’s conceptually similar to convolution, except that instead of transforming local patches via a learned linear transformation (the convolution kernel), they’re transformed via a hardcoded max tensor operation</p>
<p>A big difference from convolution is that max pooling is usually done with 2 × 2 windows and stride 2, in order to downsample the feature maps by a factor of 2. On the other hand, convolution is typically done with 3 × 3 windows and no stride (stride 1)</p>
<p>The reason to use downsampling is to reduce the number of feature-map coefficients to process, as well as to induce spatial-filter hierarchies by making successive convolution layers look at increasingly large windows (in terms of the fraction of the original input they cover)</p>
<h1 id="Training-CNN-from-scratch-on-a-small-dataset"><a href="#Training-CNN-from-scratch-on-a-small-dataset" class="headerlink" title="Training CNN from scratch on a small dataset"></a>Training CNN from scratch on a small dataset</h1><p>Trainning on large dataset is easy , but getting similar good result from small dataset is the real challange because in real life it is not easy to get a large dataset</p>
<p>Because convnets learn local, translation-invariant features, they’re highly data efficient on perceptual problems. Training a convnet from scratch on a very small image dataset will still yield reasonable results despite a relative lack of data, without the need for any custom feature engineering.</p>
<h1 id="Dataset"><a href="#Dataset" class="headerlink" title="Dataset"></a>Dataset</h1><p><a target="_blank" rel="noopener" href="https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data">https://www.kaggle.com/c/dogs-vs-cats-redux-kernels-edition/data</a></p>
<p><strong>The train folder contains 25,000 images of dogs and cats. Each image in this folder has the label as part of the filename. The test folder contains 12,500 images, named according to a numeric id. For each image in the test set, you should predict a probability that the image is a dog (1 = dog, 0 = cat).</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This Python 3 environment comes with many helpful analytics libraries installed</span></span><br><span class="line"><span class="comment"># It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python</span></span><br><span class="line"><span class="comment"># For example, here&#x27;s several helpful packages to load</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># linear algebra</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment"># data processing, CSV file I/O (e.g. pd.read_csv)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Input data files are available in the read-only &quot;../input/&quot; directory</span></span><br><span class="line"><span class="comment"># For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">for</span> dirname, _, filenames <span class="keyword">in</span> os.walk(<span class="string">&#x27;/kaggle/input&#x27;</span>):</span><br><span class="line">    <span class="keyword">for</span> filename <span class="keyword">in</span> filenames:</span><br><span class="line">        <span class="built_in">print</span>(os.path.join(dirname, filename))</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; </span></span><br><span class="line"><span class="comment"># You can also write temporary files to /kaggle/temp/, but they won&#x27;t be saved outside of the current session</span></span><br></pre></td></tr></table></figure>

<pre><code>/kaggle/input/test-save/cats_and_dogs_small_input_format_change_1.h5
/kaggle/input/test-save/cats_and_dogs_small_input_format_change_augment.h5
/kaggle/input/dogs-vs-cats-redux-kernels-edition/sample_submission.csv
/kaggle/input/dogs-vs-cats-redux-kernels-edition/train.zip
/kaggle/input/dogs-vs-cats-redux-kernels-edition/test.zip
</code></pre>
<h1 id="Extracting-zip-files"><a href="#Extracting-zip-files" class="headerlink" title="Extracting zip files"></a>Extracting zip files</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">with</span> zipfile.ZipFile(<span class="string">&quot;../input/dogs-vs-cats-redux-kernels-edition/&quot;</span>+<span class="string">&quot;train&quot;</span>+<span class="string">&quot;.zip&quot;</span>,<span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> z:</span><br><span class="line">    z.extractall(<span class="string">&quot;.&quot;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">with</span> zipfile.ZipFile(<span class="string">&quot;../input/dogs-vs-cats-redux-kernels-edition/&quot;</span>+<span class="string">&quot;test&quot;</span>+<span class="string">&quot;.zip&quot;</span>,<span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> z:</span><br><span class="line">    z.extractall(<span class="string">&quot;.&quot;</span>)</span><br></pre></td></tr></table></figure>

<h1 id="Importing-Dependencies"><a href="#Importing-Dependencies" class="headerlink" title="Importing Dependencies"></a>Importing Dependencies</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os, cv2, re, random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> img_to_array, load_img</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers, models, optimizers</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br></pre></td></tr></table></figure>

<h1 id="Preparing-Data"><a href="#Preparing-Data" class="headerlink" title="Preparing Data"></a>Preparing Data</h1><ul>
<li>Image Size - <code>(150 x 150)</code> (somewhat arbitrary)</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">img_width = <span class="number">150</span></span><br><span class="line">img_height = <span class="number">150</span></span><br><span class="line">TRAIN_DIR = <span class="string">&#x27;/kaggle/working/train/&#x27;</span></span><br><span class="line">TEST_DIR = <span class="string">&#x27;/kaggle/working/test/&#x27;</span></span><br><span class="line">train_images_dogs_cats = [TRAIN_DIR+i <span class="keyword">for</span> i <span class="keyword">in</span> os.listdir(TRAIN_DIR)] <span class="comment"># use this for full dataset</span></span><br><span class="line">test_images_dogs_cats = [TEST_DIR+i <span class="keyword">for</span> i <span class="keyword">in</span> os.listdir(TEST_DIR)]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(train_images_dogs_cats))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(test_images_dogs_cats))</span><br></pre></td></tr></table></figure>

<pre><code>25000
12500
</code></pre>
<p><strong>Some helper functions</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">atoi</span>(<span class="params">text</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">int</span>(text) <span class="keyword">if</span> text.isdigit() <span class="keyword">else</span> text</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">natural_keys</span>(<span class="params">text</span>):</span></span><br><span class="line">    <span class="keyword">return</span> [ atoi(c) <span class="keyword">for</span> c <span class="keyword">in</span> re.split(<span class="string">&#x27;(\d+)&#x27;</span>, text) ]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(natural_keys(<span class="string">&quot;cat.0.txt&quot;</span>))</span><br></pre></td></tr></table></figure>

<pre><code>[&#39;cat.&#39;, 0, &#39;.txt&#39;]
</code></pre>
<p><strong>We are not training on the ful 25k dataset . Rather, we will train on 1500 from each class (total 3000) and 20% of that for validation</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">num_of_each_sample = <span class="number">1500</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_images_dogs_cats.sort(key=natural_keys)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(train_images_dogs_cats))</span><br></pre></td></tr></table></figure>

<pre><code>25000
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_images_dogs_cats = train_images_dogs_cats[<span class="number">0</span>:num_of_each_sample] + train_images_dogs_cats[<span class="number">12500</span>:<span class="number">12500</span>+num_of_each_sample] </span><br><span class="line">test_images_dogs_cats.sort(key=natural_keys)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(train_images_dogs_cats))</span><br></pre></td></tr></table></figure>

<pre><code>3000
</code></pre>
<p><strong>More helper functions</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_data</span>(<span class="params">list_of_images</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Returns two arrays: </span></span><br><span class="line"><span class="string">        x is an array of resized images</span></span><br><span class="line"><span class="string">        y is an array of labels</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    x = [] <span class="comment"># images as arrays</span></span><br><span class="line">    y = [] <span class="comment"># labels</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> image <span class="keyword">in</span> list_of_images:</span><br><span class="line">        x.append(cv2.resize(cv2.imread(image), (img_width,img_height), interpolation=cv2.INTER_CUBIC))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> list_of_images:</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;dog&#x27;</span> <span class="keyword">in</span> i:</span><br><span class="line">            y.append(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">elif</span> <span class="string">&#x27;cat&#x27;</span> <span class="keyword">in</span> i:</span><br><span class="line">            y.append(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">#else:</span></span><br><span class="line">            <span class="comment">#print(&#x27;neither cat nor dog name present in images&#x27;)</span></span><br><span class="line">    <span class="keyword">return</span> x, y</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X, Y = prepare_data(train_images_dogs_cats)</span><br><span class="line"><span class="built_in">print</span>(K.image_data_format())</span><br></pre></td></tr></table></figure>

<pre><code>channels_last
</code></pre>
<h1 id="Train-Validation-Split"><a href="#Train-Validation-Split" class="headerlink" title="Train Validation Split"></a>Train Validation Split</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># First split the data in two sets, 80% for training, 20% for Val/Test)</span></span><br><span class="line">X_train, X_val, Y_train, Y_val = train_test_split(X,Y, test_size=<span class="number">0.2</span>, random_state=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nb_train_samples = <span class="built_in">len</span>(X_train)</span><br><span class="line">nb_validation_samples = <span class="built_in">len</span>(X_val)</span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(nb_train_samples)</span><br><span class="line"><span class="built_in">print</span>(nb_validation_samples)</span><br></pre></td></tr></table></figure>

<pre><code>2400
600
</code></pre>
<h1 id="Building-the-CNN"><a href="#Building-the-CNN" class="headerlink" title="Building the CNN"></a>Building the CNN</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>, input_shape=(img_width, img_height, <span class="number">3</span>)))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dense(<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 148, 148, 32)      896       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 74, 74, 32)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 72, 72, 64)        18496     
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 34, 34, 128)       73856     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 17, 17, 128)       0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 15, 15, 128)       147584    
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         
_________________________________________________________________
flatten (Flatten)            (None, 6272)              0         
_________________________________________________________________
dense (Dense)                (None, 512)               3211776   
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 513       
=================================================================
Total params: 3,453,121
Trainable params: 3,453,121
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>,</span><br><span class="line">              optimizer=optimizers.RMSprop(lr=<span class="number">1e-4</span>),</span><br><span class="line">              metrics=[<span class="string">&#x27;acc&#x27;</span>])</span><br></pre></td></tr></table></figure>

<h1 id="Data-preprocessing"><a href="#Data-preprocessing" class="headerlink" title="Data preprocessing"></a>Data preprocessing</h1><p>As you already know by now, data should be formatted into appropriately pre-processed floating point tensors before being fed into our network. Currently, our data sits on a drive as JPEG files, so the steps for getting it into our network are roughly:</p>
<ul>
<li><strong>Read the picture files.</strong></li>
<li><strong>Decode the JPEG content to RBG grids of pixels.</strong></li>
<li><strong>Convert these into floating point tensors.</strong></li>
<li><strong>Rescale the pixel values (between 0 and 255) to the <code>[0, 1]</code> interval (as you know, neural networks prefer to deal with small input values).</strong></li>
</ul>
<p>It may seem a bit daunting, but thankfully Keras has utilities to take care of these steps automatically. Keras has a module with image processing helper tools, located at keras.preprocessing.image. In particular, it contains the class ImageDataGenerator which allows to quickly set up Python generators that can automatically turn image files on disk into batches of pre-processed tensors. This is what we will use here</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># All images will be rescaled by 1./255</span></span><br><span class="line"></span><br><span class="line">train_datagen = ImageDataGenerator(rescale=<span class="number">1.</span> / <span class="number">255</span>)</span><br><span class="line">test_datagen = ImageDataGenerator(rescale=<span class="number">1.</span>/<span class="number">255</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_generator = train_datagen.flow(np.array(X_train), Y_train, batch_size=batch_size)</span><br><span class="line">validation_generator = test_datagen.flow(np.array(X_val), Y_val, batch_size=batch_size)</span><br></pre></td></tr></table></figure>

<h1 id="Train-Fit"><a href="#Train-Fit" class="headerlink" title="Train / Fit"></a>Train / Fit</h1><p>Let’s fit our model to the data using the generator. We do it using the <code>fit_generator</code> method, the equivalent of fit for data generators like ours.</p>
<p>It expects as first argument a <em>Python generator</em> that will yield batches of inputs and targets indefinitely, like ours does. Because the data is being generated endlessly, the generator needs to know example <em>how many samples to draw from the generator before declaring an epoch over</em>. This is the role of the <code>steps_per_epoch</code> argument: <em>after having drawn steps_per_epoch batches from the generator, i.e. after having run for steps_per_epoch gradient descent steps, the fitting process will go to the next epoch</em></p>
<p>So , <code>steps_per_epoch</code> = <code>np.ceil(number_of_samples/batch_size)</code></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit_generator(</span><br><span class="line">    train_generator, </span><br><span class="line">    steps_per_epoch=np.ceil(nb_train_samples/batch_size),</span><br><span class="line">    epochs=<span class="number">30</span>,</span><br><span class="line">    validation_data=validation_generator,</span><br><span class="line">    validation_steps=np.ceil(nb_validation_samples/batch_size)</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/30
150/150 [==============================] - 2s 14ms/step - loss: 0.6928 - acc: 0.5246 - val_loss: 0.6863 - val_acc: 0.5133
Epoch 2/30
150/150 [==============================] - 2s 12ms/step - loss: 0.6583 - acc: 0.6129 - val_loss: 0.6577 - val_acc: 0.5650
Epoch 3/30
150/150 [==============================] - 2s 13ms/step - loss: 0.6234 - acc: 0.6496 - val_loss: 0.5908 - val_acc: 0.7067
Epoch 4/30
150/150 [==============================] - 2s 13ms/step - loss: 0.5805 - acc: 0.6967 - val_loss: 0.5614 - val_acc: 0.7117
Epoch 5/30
150/150 [==============================] - 2s 13ms/step - loss: 0.5432 - acc: 0.7283 - val_loss: 0.5243 - val_acc: 0.7583
Epoch 6/30
150/150 [==============================] - 2s 13ms/step - loss: 0.5048 - acc: 0.7558 - val_loss: 0.5174 - val_acc: 0.7417
Epoch 7/30
150/150 [==============================] - 2s 12ms/step - loss: 0.4759 - acc: 0.7708 - val_loss: 0.5186 - val_acc: 0.7400
Epoch 8/30
150/150 [==============================] - 2s 12ms/step - loss: 0.4391 - acc: 0.7942 - val_loss: 0.4720 - val_acc: 0.7850
Epoch 9/30
150/150 [==============================] - 2s 13ms/step - loss: 0.4030 - acc: 0.8183 - val_loss: 0.4758 - val_acc: 0.7833
Epoch 10/30
150/150 [==============================] - 2s 12ms/step - loss: 0.3795 - acc: 0.8283 - val_loss: 0.4686 - val_acc: 0.7667
Epoch 11/30
150/150 [==============================] - 2s 13ms/step - loss: 0.3461 - acc: 0.8554 - val_loss: 0.4711 - val_acc: 0.7700
Epoch 12/30
150/150 [==============================] - 2s 12ms/step - loss: 0.3205 - acc: 0.8579 - val_loss: 0.4720 - val_acc: 0.7817
Epoch 13/30
150/150 [==============================] - 2s 12ms/step - loss: 0.2980 - acc: 0.8750 - val_loss: 0.5153 - val_acc: 0.7617
Epoch 14/30
150/150 [==============================] - 2s 13ms/step - loss: 0.2590 - acc: 0.9000 - val_loss: 0.5618 - val_acc: 0.7600
Epoch 15/30
150/150 [==============================] - 2s 12ms/step - loss: 0.2253 - acc: 0.9150 - val_loss: 0.5242 - val_acc: 0.7617
Epoch 16/30
150/150 [==============================] - 2s 15ms/step - loss: 0.2100 - acc: 0.9187 - val_loss: 0.5096 - val_acc: 0.7717
Epoch 17/30
150/150 [==============================] - 2s 12ms/step - loss: 0.1727 - acc: 0.9396 - val_loss: 0.5207 - val_acc: 0.7683
Epoch 18/30
150/150 [==============================] - 2s 12ms/step - loss: 0.1538 - acc: 0.9463 - val_loss: 0.5911 - val_acc: 0.7600
Epoch 19/30
150/150 [==============================] - 2s 13ms/step - loss: 0.1271 - acc: 0.9579 - val_loss: 0.6003 - val_acc: 0.7767
Epoch 20/30
150/150 [==============================] - 2s 12ms/step - loss: 0.1086 - acc: 0.9663 - val_loss: 0.6471 - val_acc: 0.7600
Epoch 21/30
150/150 [==============================] - 2s 12ms/step - loss: 0.0920 - acc: 0.9688 - val_loss: 0.7451 - val_acc: 0.7450
Epoch 22/30
150/150 [==============================] - 3s 17ms/step - loss: 0.0736 - acc: 0.9767 - val_loss: 0.7504 - val_acc: 0.7533
Epoch 23/30
150/150 [==============================] - 2s 13ms/step - loss: 0.0676 - acc: 0.9792 - val_loss: 0.7973 - val_acc: 0.7400
Epoch 24/30
150/150 [==============================] - 2s 12ms/step - loss: 0.0551 - acc: 0.9858 - val_loss: 0.7448 - val_acc: 0.7683
Epoch 25/30
150/150 [==============================] - 2s 12ms/step - loss: 0.0434 - acc: 0.9892 - val_loss: 1.0532 - val_acc: 0.7483
Epoch 26/30
150/150 [==============================] - 2s 12ms/step - loss: 0.0384 - acc: 0.9908 - val_loss: 0.8506 - val_acc: 0.7583
Epoch 27/30
150/150 [==============================] - 2s 13ms/step - loss: 0.0332 - acc: 0.9900 - val_loss: 0.8640 - val_acc: 0.7700
Epoch 28/30
150/150 [==============================] - 2s 13ms/step - loss: 0.0319 - acc: 0.9912 - val_loss: 0.8931 - val_acc: 0.7650
Epoch 29/30
150/150 [==============================] - 2s 13ms/step - loss: 0.0239 - acc: 0.9917 - val_loss: 1.0431 - val_acc: 0.7667
Epoch 30/30
150/150 [==============================] - 2s 12ms/step - loss: 0.0197 - acc: 0.9942 - val_loss: 1.1201 - val_acc: 0.7383
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.save(<span class="string">&#x27;cats_and_dogs_small_input_format_change_1.h5&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">acc = history.history[<span class="string">&#x27;acc&#x27;</span>]</span><br><span class="line">val_acc = history.history[<span class="string">&#x27;val_acc&#x27;</span>]</span><br><span class="line">loss = history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">val_loss = history.history[<span class="string">&#x27;val_loss&#x27;</span>]</span><br><span class="line"></span><br><span class="line">epochs = <span class="built_in">range</span>(<span class="built_in">len</span>(acc))</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, acc, <span class="string">&#x27;bo&#x27;</span>, label=<span class="string">&#x27;Training acc&#x27;</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;Validation acc&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and validation accuracy&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, loss, <span class="string">&#x27;bo&#x27;</span>, label=<span class="string">&#x27;Training loss&#x27;</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;Validation loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and validation loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>


<p><img src="output_40_0.png" alt="png"></p>
<p><img src="output_40_1.png" alt="png"></p>
<p>These plots are characteristic of overfitting. Our training accuracy increases linearly over time, until it reaches nearly 100%, while our validation accuracy stalls at 73-75%. Our validation loss reaches its minimum after only five epochs then stalls, while the training loss keeps decreasing linearly until it reaches nearly 0</p>
<p>Because we only have relatively few training samples (2400), overfitting is going to be our number one concern. You already know about a number of techniques that can help mitigate overfitting, such as <code>dropout</code> and <code>weight decay (L2 regularization)</code> . We are now going to introduce a new one, specific to computer vision, and used almost universally when processing images with deep learning models: <code>data augmentation</code>.</p>
<h1 id="Predict"><a href="#Predict" class="headerlink" title="Predict"></a>Predict</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(test_images_dogs_cats))</span><br><span class="line">X_test, Y_test = prepare_data(test_images_dogs_cats) <span class="comment">#Y_test in this case will be []</span></span><br></pre></td></tr></table></figure>

<pre><code>12500
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model</span><br><span class="line">model = load_model(<span class="string">&#x27;/kaggle/input/test-save/cats_and_dogs_small_input_format_change_1.h5&#x27;</span>)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d (Conv2D)              (None, 148, 148, 32)      896       
_________________________________________________________________
max_pooling2d (MaxPooling2D) (None, 74, 74, 32)        0         
_________________________________________________________________
conv2d_1 (Conv2D)            (None, 72, 72, 64)        18496     
_________________________________________________________________
max_pooling2d_1 (MaxPooling2 (None, 36, 36, 64)        0         
_________________________________________________________________
conv2d_2 (Conv2D)            (None, 34, 34, 128)       73856     
_________________________________________________________________
max_pooling2d_2 (MaxPooling2 (None, 17, 17, 128)       0         
_________________________________________________________________
conv2d_3 (Conv2D)            (None, 15, 15, 128)       147584    
_________________________________________________________________
max_pooling2d_3 (MaxPooling2 (None, 7, 7, 128)         0         
_________________________________________________________________
flatten (Flatten)            (None, 6272)              0         
_________________________________________________________________
dense (Dense)                (None, 512)               3211776   
_________________________________________________________________
dense_1 (Dense)              (None, 1)                 513       
=================================================================
Total params: 3,453,121
Trainable params: 3,453,121
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test_generator = test_datagen.flow(np.array(X_test), batch_size=batch_size)</span><br><span class="line">prediction_probabilities = model.predict_generator(test_generator, verbose=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<pre><code>782/782 [==============================] - 4s 5ms/step
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(prediction_probabilities.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(12500, 1)
</code></pre>
<h1 id="Creating-Submission-File"><a href="#Creating-Submission-File" class="headerlink" title="Creating Submission File"></a>Creating Submission File</h1><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">counter = <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(test_images_dogs_cats) + <span class="number">1</span>)</span><br><span class="line">solution = pd.DataFrame(&#123;<span class="string">&quot;id&quot;</span>: counter, <span class="string">&quot;label&quot;</span>:<span class="built_in">list</span>(prediction_probabilities)&#125;)</span><br><span class="line">cols = [<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> cols:</span><br><span class="line">    solution[col] = solution[col].<span class="built_in">map</span>(<span class="keyword">lambda</span> x: <span class="built_in">str</span>(x).lstrip(<span class="string">&#x27;[&#x27;</span>).rstrip(<span class="string">&#x27;]&#x27;</span>)).astype(<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">solution.to_csv(<span class="string">&quot;dogsVScats2.csv&quot;</span>, index = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<h1 id="Data-Augmentation"><a href="#Data-Augmentation" class="headerlink" title="Data Augmentation"></a>Data Augmentation</h1><p><strong>Overfitting is caused by having too few samples to learn from, rendering us unable to train a model able to generalize to new data. Given infinite data, our model would be exposed to every possible aspect of the data distribution at hand: we would never overfit. Data augmentation takes the approach of generating more training data from existing training samples, by “augmenting” the samples via a number of random transformations that yield believable-looking images. The goal is that at training time, our model would never see the exact same picture twice. This helps the model get exposed to more aspects of the data and generalize better.</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Conv2D(<span class="number">32</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>,</span><br><span class="line">                        input_shape=(<span class="number">150</span>, <span class="number">150</span>, <span class="number">3</span>)))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">64</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Conv2D(<span class="number">128</span>, (<span class="number">3</span>, <span class="number">3</span>), activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(layers.MaxPooling2D((<span class="number">2</span>, <span class="number">2</span>)))</span><br><span class="line">model.add(layers.Flatten())</span><br><span class="line">model.add(layers.Dropout(<span class="number">0.5</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">512</span>, activation=<span class="string">&#x27;relu&#x27;</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>,</span><br><span class="line">              optimizer=optimizers.RMSprop(lr=<span class="number">1e-4</span>),</span><br><span class="line">              metrics=[<span class="string">&#x27;acc&#x27;</span>])</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">train_datagen = ImageDataGenerator(</span><br><span class="line">    rescale=<span class="number">1.</span>/<span class="number">255</span>,</span><br><span class="line">    rotation_range=<span class="number">40</span>,</span><br><span class="line">    width_shift_range=<span class="number">0.2</span>,</span><br><span class="line">    height_shift_range=<span class="number">0.2</span>,</span><br><span class="line">    shear_range=<span class="number">0.2</span>,</span><br><span class="line">    zoom_range=<span class="number">0.2</span>,</span><br><span class="line">    horizontal_flip=<span class="literal">True</span>,)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Note that the validation data should not be augmented!</span></span><br><span class="line">test_datagen = ImageDataGenerator(rescale=<span class="number">1.</span>/<span class="number">255</span>)</span><br></pre></td></tr></table></figure>



<p>These are just a few of the options available (for more, see the Keras documentation). Let’s quickly go over what we just wrote:</p>
<ul>
<li><code>rotation_range</code> is a value in degrees <code>(0-180)</code>, a range within which to randomly rotate pictures.</li>
<li><code>width_shift</code> and <code>height_shift</code> are ranges (as a fraction of total width or height) within which to randomly translate pictures vertically or horizontally.</li>
<li><code>shear_range</code> is for randomly applying shearing transformations.</li>
<li><code>zoom_range</code> is for randomly zooming inside pictures.</li>
<li><code>horizontal_flip</code> is for randomly flipping half of the images horizontally – relevant when there are no assumptions of horizontal asymmetry (e.g. real-world pictures).</li>
<li><code>fill_mode</code> is the strategy used for filling in newly created pixels, which can appear after a rotation or a width/height shift.</li>
</ul>
<h3 id="Note"><a href="#Note" class="headerlink" title="Note"></a><strong>Note</strong></h3><p>validation data should not be augmented</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_generator = train_datagen.flow(np.array(X_train), Y_train, batch_size=batch_size)</span><br><span class="line">validation_generator = test_datagen.flow(np.array(X_val), Y_val, batch_size=batch_size)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">history = model.fit_generator(</span><br><span class="line">      train_generator,</span><br><span class="line">      steps_per_epoch=np.ceil(nb_train_samples/batch_size),</span><br><span class="line">      epochs=<span class="number">100</span>,</span><br><span class="line">      validation_data=validation_generator,</span><br><span class="line">      validation_steps=np.ceil(nb_validation_samples/batch_size))</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/100
150/150 [==============================] - 12s 83ms/step - loss: 0.6913 - acc: 0.5317 - val_loss: 0.6714 - val_acc: 0.5983
Epoch 2/100
150/150 [==============================] - 13s 84ms/step - loss: 0.6788 - acc: 0.5700 - val_loss: 0.6493 - val_acc: 0.6267
Epoch 3/100
150/150 [==============================] - 12s 81ms/step - loss: 0.6621 - acc: 0.5925 - val_loss: 0.6307 - val_acc: 0.6183
Epoch 4/100
150/150 [==============================] - 12s 81ms/step - loss: 0.6475 - acc: 0.6133 - val_loss: 0.6504 - val_acc: 0.5850
Epoch 5/100
150/150 [==============================] - 14s 92ms/step - loss: 0.6345 - acc: 0.6267 - val_loss: 0.6253 - val_acc: 0.6267
Epoch 6/100
150/150 [==============================] - 12s 80ms/step - loss: 0.6183 - acc: 0.6579 - val_loss: 0.5527 - val_acc: 0.7067
Epoch 7/100
150/150 [==============================] - 13s 86ms/step - loss: 0.6148 - acc: 0.6488 - val_loss: 0.5648 - val_acc: 0.7067
Epoch 8/100
150/150 [==============================] - 12s 82ms/step - loss: 0.6049 - acc: 0.6733 - val_loss: 0.5266 - val_acc: 0.7517
Epoch 9/100
150/150 [==============================] - 12s 81ms/step - loss: 0.5902 - acc: 0.6842 - val_loss: 0.5117 - val_acc: 0.7450
Epoch 10/100
150/150 [==============================] - 14s 90ms/step - loss: 0.5824 - acc: 0.6938 - val_loss: 0.5332 - val_acc: 0.7300
Epoch 11/100
150/150 [==============================] - 12s 80ms/step - loss: 0.5740 - acc: 0.6988 - val_loss: 0.4963 - val_acc: 0.7633
Epoch 12/100
150/150 [==============================] - 13s 84ms/step - loss: 0.5878 - acc: 0.6913 - val_loss: 0.5169 - val_acc: 0.7383
Epoch 13/100
150/150 [==============================] - 13s 87ms/step - loss: 0.5559 - acc: 0.7142 - val_loss: 0.5108 - val_acc: 0.7467
Epoch 14/100
150/150 [==============================] - 13s 83ms/step - loss: 0.5529 - acc: 0.7237 - val_loss: 0.5053 - val_acc: 0.7550
Epoch 15/100
150/150 [==============================] - 14s 91ms/step - loss: 0.5610 - acc: 0.7075 - val_loss: 0.4721 - val_acc: 0.7717
Epoch 16/100
150/150 [==============================] - 12s 79ms/step - loss: 0.5489 - acc: 0.7163 - val_loss: 0.4639 - val_acc: 0.7683
Epoch 17/100
150/150 [==============================] - 13s 84ms/step - loss: 0.5369 - acc: 0.7292 - val_loss: 0.4881 - val_acc: 0.7500
Epoch 18/100
150/150 [==============================] - 13s 85ms/step - loss: 0.5464 - acc: 0.7321 - val_loss: 0.4489 - val_acc: 0.7817
Epoch 19/100
150/150 [==============================] - 13s 84ms/step - loss: 0.5376 - acc: 0.7375 - val_loss: 0.4577 - val_acc: 0.7750
Epoch 20/100
150/150 [==============================] - 14s 92ms/step - loss: 0.5441 - acc: 0.7233 - val_loss: 0.4655 - val_acc: 0.7733
Epoch 21/100
150/150 [==============================] - 12s 79ms/step - loss: 0.5329 - acc: 0.7258 - val_loss: 0.4382 - val_acc: 0.7950
Epoch 22/100
150/150 [==============================] - 13s 86ms/step - loss: 0.5405 - acc: 0.7292 - val_loss: 0.4337 - val_acc: 0.7933
Epoch 23/100
150/150 [==============================] - 14s 91ms/step - loss: 0.5170 - acc: 0.7379 - val_loss: 0.4228 - val_acc: 0.8017
Epoch 24/100
150/150 [==============================] - 13s 89ms/step - loss: 0.5247 - acc: 0.7450 - val_loss: 0.4446 - val_acc: 0.7833
Epoch 25/100
150/150 [==============================] - 13s 88ms/step - loss: 0.5146 - acc: 0.7483 - val_loss: 0.4597 - val_acc: 0.7850
Epoch 26/100
150/150 [==============================] - 12s 80ms/step - loss: 0.5237 - acc: 0.7529 - val_loss: 0.4326 - val_acc: 0.7917
Epoch 27/100
150/150 [==============================] - 13s 89ms/step - loss: 0.5112 - acc: 0.7446 - val_loss: 0.4885 - val_acc: 0.7633
Epoch 28/100
150/150 [==============================] - 13s 88ms/step - loss: 0.5107 - acc: 0.7521 - val_loss: 0.4418 - val_acc: 0.7967
Epoch 29/100
150/150 [==============================] - 13s 87ms/step - loss: 0.5008 - acc: 0.7563 - val_loss: 0.4096 - val_acc: 0.8050
Epoch 30/100
150/150 [==============================] - 14s 91ms/step - loss: 0.4918 - acc: 0.7588 - val_loss: 0.4319 - val_acc: 0.8050
Epoch 31/100
150/150 [==============================] - 12s 79ms/step - loss: 0.5029 - acc: 0.7550 - val_loss: 0.4339 - val_acc: 0.7983
Epoch 32/100
150/150 [==============================] - 13s 87ms/step - loss: 0.4960 - acc: 0.7675 - val_loss: 0.4259 - val_acc: 0.8150
Epoch 33/100
150/150 [==============================] - 13s 88ms/step - loss: 0.4961 - acc: 0.7483 - val_loss: 0.4117 - val_acc: 0.8033
Epoch 34/100
150/150 [==============================] - 12s 83ms/step - loss: 0.4986 - acc: 0.7633 - val_loss: 0.4326 - val_acc: 0.7967
Epoch 35/100
150/150 [==============================] - 14s 90ms/step - loss: 0.4892 - acc: 0.7646 - val_loss: 0.4050 - val_acc: 0.8217
Epoch 36/100
150/150 [==============================] - 12s 79ms/step - loss: 0.4891 - acc: 0.7646 - val_loss: 0.4288 - val_acc: 0.8100
Epoch 37/100
150/150 [==============================] - 13s 86ms/step - loss: 0.4780 - acc: 0.7704 - val_loss: 0.4018 - val_acc: 0.8083
Epoch 38/100
150/150 [==============================] - 14s 94ms/step - loss: 0.4876 - acc: 0.7617 - val_loss: 0.4317 - val_acc: 0.7817
Epoch 39/100
150/150 [==============================] - 13s 83ms/step - loss: 0.4804 - acc: 0.7721 - val_loss: 0.3901 - val_acc: 0.8217
Epoch 40/100
150/150 [==============================] - 14s 94ms/step - loss: 0.4748 - acc: 0.7729 - val_loss: 0.4204 - val_acc: 0.7967
Epoch 41/100
150/150 [==============================] - 13s 84ms/step - loss: 0.4834 - acc: 0.7758 - val_loss: 0.4114 - val_acc: 0.8017
Epoch 42/100
150/150 [==============================] - 14s 93ms/step - loss: 0.4674 - acc: 0.7763 - val_loss: 0.4137 - val_acc: 0.8067
Epoch 43/100
150/150 [==============================] - 14s 94ms/step - loss: 0.4671 - acc: 0.7842 - val_loss: 0.4207 - val_acc: 0.7967
Epoch 44/100
150/150 [==============================] - 13s 87ms/step - loss: 0.4643 - acc: 0.7896 - val_loss: 0.3832 - val_acc: 0.8183
Epoch 45/100
150/150 [==============================] - 14s 97ms/step - loss: 0.4554 - acc: 0.7846 - val_loss: 0.5359 - val_acc: 0.7700
Epoch 46/100
150/150 [==============================] - 14s 90ms/step - loss: 0.4634 - acc: 0.7767 - val_loss: 0.3861 - val_acc: 0.8250
Epoch 47/100
150/150 [==============================] - 12s 79ms/step - loss: 0.4548 - acc: 0.7821 - val_loss: 0.4177 - val_acc: 0.8067
Epoch 48/100
150/150 [==============================] - 14s 92ms/step - loss: 0.4544 - acc: 0.7921 - val_loss: 0.4165 - val_acc: 0.8100
Epoch 49/100
150/150 [==============================] - 13s 84ms/step - loss: 0.4559 - acc: 0.7837 - val_loss: 0.3776 - val_acc: 0.8367
Epoch 50/100
150/150 [==============================] - 14s 91ms/step - loss: 0.4528 - acc: 0.7917 - val_loss: 0.3620 - val_acc: 0.8417
Epoch 51/100
150/150 [==============================] - 13s 90ms/step - loss: 0.4499 - acc: 0.7858 - val_loss: 0.4067 - val_acc: 0.8233
Epoch 52/100
150/150 [==============================] - 12s 78ms/step - loss: 0.4483 - acc: 0.8004 - val_loss: 0.3932 - val_acc: 0.8233
Epoch 53/100
150/150 [==============================] - 15s 98ms/step - loss: 0.4530 - acc: 0.7946 - val_loss: 0.4569 - val_acc: 0.7750
Epoch 54/100
150/150 [==============================] - 12s 79ms/step - loss: 0.4479 - acc: 0.7962 - val_loss: 0.3650 - val_acc: 0.8367
Epoch 55/100
150/150 [==============================] - 14s 91ms/step - loss: 0.4396 - acc: 0.7992 - val_loss: 0.3847 - val_acc: 0.8250
Epoch 56/100
150/150 [==============================] - 13s 89ms/step - loss: 0.4324 - acc: 0.7967 - val_loss: 0.4703 - val_acc: 0.7917
Epoch 57/100
150/150 [==============================] - 12s 81ms/step - loss: 0.4382 - acc: 0.7975 - val_loss: 0.3553 - val_acc: 0.8500
Epoch 58/100
150/150 [==============================] - 15s 100ms/step - loss: 0.4264 - acc: 0.8083 - val_loss: 0.3588 - val_acc: 0.8400
Epoch 59/100
150/150 [==============================] - 12s 78ms/step - loss: 0.4220 - acc: 0.8163 - val_loss: 0.3755 - val_acc: 0.8333
Epoch 60/100
150/150 [==============================] - 13s 85ms/step - loss: 0.4356 - acc: 0.8112 - val_loss: 0.3613 - val_acc: 0.8300
Epoch 61/100
150/150 [==============================] - 15s 100ms/step - loss: 0.4293 - acc: 0.8087 - val_loss: 0.3683 - val_acc: 0.8333
Epoch 62/100
150/150 [==============================] - 12s 79ms/step - loss: 0.4221 - acc: 0.8125 - val_loss: 0.3842 - val_acc: 0.8200
Epoch 63/100
150/150 [==============================] - 14s 97ms/step - loss: 0.4298 - acc: 0.8079 - val_loss: 0.3976 - val_acc: 0.8267
Epoch 64/100
150/150 [==============================] - 13s 86ms/step - loss: 0.4262 - acc: 0.8050 - val_loss: 0.4451 - val_acc: 0.7867
Epoch 65/100
150/150 [==============================] - 13s 83ms/step - loss: 0.4156 - acc: 0.8067 - val_loss: 0.3641 - val_acc: 0.8250
Epoch 66/100
150/150 [==============================] - 14s 96ms/step - loss: 0.4161 - acc: 0.8054 - val_loss: 0.3554 - val_acc: 0.8400
Epoch 67/100
150/150 [==============================] - 12s 79ms/step - loss: 0.4098 - acc: 0.8158 - val_loss: 0.4377 - val_acc: 0.8167
Epoch 68/100
150/150 [==============================] - 13s 87ms/step - loss: 0.4072 - acc: 0.8200 - val_loss: 0.3489 - val_acc: 0.8350
Epoch 69/100
150/150 [==============================] - 15s 103ms/step - loss: 0.4057 - acc: 0.8142 - val_loss: 0.3842 - val_acc: 0.8183
Epoch 70/100
150/150 [==============================] - 12s 82ms/step - loss: 0.4007 - acc: 0.8150 - val_loss: 0.3725 - val_acc: 0.8367
Epoch 71/100
150/150 [==============================] - 13s 87ms/step - loss: 0.4097 - acc: 0.8192 - val_loss: 0.3371 - val_acc: 0.8367
Epoch 72/100
150/150 [==============================] - 13s 88ms/step - loss: 0.3939 - acc: 0.8183 - val_loss: 0.3465 - val_acc: 0.8450
Epoch 73/100
150/150 [==============================] - 13s 86ms/step - loss: 0.4022 - acc: 0.8146 - val_loss: 0.3362 - val_acc: 0.8550
Epoch 74/100
150/150 [==============================] - 15s 98ms/step - loss: 0.4003 - acc: 0.8271 - val_loss: 0.3516 - val_acc: 0.8500
Epoch 75/100
150/150 [==============================] - 13s 83ms/step - loss: 0.3979 - acc: 0.8221 - val_loss: 0.3615 - val_acc: 0.8383
Epoch 76/100
150/150 [==============================] - 12s 79ms/step - loss: 0.3997 - acc: 0.8271 - val_loss: 0.4037 - val_acc: 0.8200
Epoch 77/100
150/150 [==============================] - 16s 104ms/step - loss: 0.3882 - acc: 0.8179 - val_loss: 0.3508 - val_acc: 0.8550
Epoch 78/100
150/150 [==============================] - 12s 82ms/step - loss: 0.3911 - acc: 0.8308 - val_loss: 0.4287 - val_acc: 0.8200
Epoch 79/100
150/150 [==============================] - 13s 86ms/step - loss: 0.4035 - acc: 0.8192 - val_loss: 0.3530 - val_acc: 0.8383
Epoch 80/100
150/150 [==============================] - 14s 94ms/step - loss: 0.3823 - acc: 0.8250 - val_loss: 0.3689 - val_acc: 0.8500
Epoch 81/100
150/150 [==============================] - 12s 78ms/step - loss: 0.4039 - acc: 0.8138 - val_loss: 0.3573 - val_acc: 0.8417
Epoch 82/100
150/150 [==============================] - 17s 112ms/step - loss: 0.3857 - acc: 0.8254 - val_loss: 0.3612 - val_acc: 0.8417
Epoch 83/100
150/150 [==============================] - 12s 79ms/step - loss: 0.3851 - acc: 0.8304 - val_loss: 0.3506 - val_acc: 0.8433
Epoch 84/100
150/150 [==============================] - 12s 79ms/step - loss: 0.3875 - acc: 0.8342 - val_loss: 0.3266 - val_acc: 0.8567
Epoch 85/100
150/150 [==============================] - 12s 82ms/step - loss: 0.3790 - acc: 0.8325 - val_loss: 0.3239 - val_acc: 0.8750
Epoch 86/100
150/150 [==============================] - 12s 78ms/step - loss: 0.3691 - acc: 0.8354 - val_loss: 0.3252 - val_acc: 0.8683
Epoch 87/100
150/150 [==============================] - 15s 102ms/step - loss: 0.3833 - acc: 0.8296 - val_loss: 0.3486 - val_acc: 0.8500
Epoch 88/100
150/150 [==============================] - 13s 86ms/step - loss: 0.3758 - acc: 0.8275 - val_loss: 0.4556 - val_acc: 0.7933
Epoch 89/100
150/150 [==============================] - 12s 78ms/step - loss: 0.3875 - acc: 0.8275 - val_loss: 0.3343 - val_acc: 0.8583
Epoch 90/100
150/150 [==============================] - 12s 82ms/step - loss: 0.3691 - acc: 0.8446 - val_loss: 0.3399 - val_acc: 0.8533
Epoch 91/100
150/150 [==============================] - 13s 83ms/step - loss: 0.3715 - acc: 0.8408 - val_loss: 0.3991 - val_acc: 0.8117
Epoch 92/100
150/150 [==============================] - 14s 91ms/step - loss: 0.3627 - acc: 0.8400 - val_loss: 0.3537 - val_acc: 0.8400
Epoch 93/100
150/150 [==============================] - 15s 98ms/step - loss: 0.3830 - acc: 0.8296 - val_loss: 0.3945 - val_acc: 0.8350
Epoch 94/100
150/150 [==============================] - 12s 78ms/step - loss: 0.3808 - acc: 0.8325 - val_loss: 0.3871 - val_acc: 0.8283
Epoch 95/100
150/150 [==============================] - 12s 83ms/step - loss: 0.3621 - acc: 0.8396 - val_loss: 0.4357 - val_acc: 0.8183
Epoch 96/100
150/150 [==============================] - 13s 85ms/step - loss: 0.3833 - acc: 0.8371 - val_loss: 0.3107 - val_acc: 0.8800
Epoch 97/100
150/150 [==============================] - 13s 84ms/step - loss: 0.3619 - acc: 0.8383 - val_loss: 0.4303 - val_acc: 0.8250
Epoch 98/100
150/150 [==============================] - 16s 104ms/step - loss: 0.3667 - acc: 0.8358 - val_loss: 0.4421 - val_acc: 0.8233
Epoch 99/100
150/150 [==============================] - 12s 82ms/step - loss: 0.3714 - acc: 0.8321 - val_loss: 0.4067 - val_acc: 0.8267
Epoch 100/100
150/150 [==============================] - 12s 83ms/step - loss: 0.3736 - acc: 0.8296 - val_loss: 0.3370 - val_acc: 0.8550
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.save(<span class="string">&#x27;cats_and_dogs_small_input_format_change_augment.h5&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model</span><br><span class="line">model = load_model(<span class="string">&#x27;/kaggle/input/test-save/cats_and_dogs_small_input_format_change_augment.h5&#x27;</span>)</span><br><span class="line">model.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;sequential_2&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
conv2d_8 (Conv2D)            (None, 148, 148, 32)      896       
_________________________________________________________________
max_pooling2d_8 (MaxPooling2 (None, 74, 74, 32)        0         
_________________________________________________________________
conv2d_9 (Conv2D)            (None, 72, 72, 64)        18496     
_________________________________________________________________
max_pooling2d_9 (MaxPooling2 (None, 36, 36, 64)        0         
_________________________________________________________________
conv2d_10 (Conv2D)           (None, 34, 34, 128)       73856     
_________________________________________________________________
max_pooling2d_10 (MaxPooling (None, 17, 17, 128)       0         
_________________________________________________________________
conv2d_11 (Conv2D)           (None, 15, 15, 128)       147584    
_________________________________________________________________
max_pooling2d_11 (MaxPooling (None, 7, 7, 128)         0         
_________________________________________________________________
flatten_2 (Flatten)          (None, 6272)              0         
_________________________________________________________________
dropout_1 (Dropout)          (None, 6272)              0         
_________________________________________________________________
dense_4 (Dense)              (None, 512)               3211776   
_________________________________________________________________
dense_5 (Dense)              (None, 1)                 513       
=================================================================
Total params: 3,453,121
Trainable params: 3,453,121
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">test_generator = test_datagen.flow(np.array(X_test), batch_size=batch_size)</span><br><span class="line"></span><br><span class="line">prediction_probabilities = model.predict_generator(test_generator, verbose=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(prediction_probabilities.shape)</span><br></pre></td></tr></table></figure>

<pre><code>782/782 [==============================] - 4s 5ms/step
(12500, 1)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">counter = <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(test_images_dogs_cats) + <span class="number">1</span>)</span><br><span class="line">solution = pd.DataFrame(&#123;<span class="string">&quot;id&quot;</span>: counter, <span class="string">&quot;label&quot;</span>:<span class="built_in">list</span>(prediction_probabilities)&#125;)</span><br><span class="line">cols = [<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> cols:</span><br><span class="line">    solution[col] = solution[col].<span class="built_in">map</span>(<span class="keyword">lambda</span> x: <span class="built_in">str</span>(x).lstrip(<span class="string">&#x27;[&#x27;</span>).rstrip(<span class="string">&#x27;]&#x27;</span>)).astype(<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">solution.to_csv(<span class="string">&quot;dogsVScats2.csv&quot;</span>, index = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></table></figure>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/mindcraft/">Home</a></li>
         
          <li><a href="/mindcraft/archives/">Archive</a></li>
         
          <li><a href="https://zarif98sjs.github.io/">About Me</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Introduction"><span class="toc-number">1.</span> <span class="toc-text">Introduction</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Properties"><span class="toc-number">2.</span> <span class="toc-text">Properties</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Feature-Maps"><span class="toc-number">3.</span> <span class="toc-text">Feature Maps</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Filters"><span class="toc-number">4.</span> <span class="toc-text">Filters</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Parameters"><span class="toc-number">5.</span> <span class="toc-text">Parameters</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Convolution-Mechanism"><span class="toc-number">6.</span> <span class="toc-text">Convolution Mechanism</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Max-Pooling"><span class="toc-number">7.</span> <span class="toc-text">Max Pooling</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Training-CNN-from-scratch-on-a-small-dataset"><span class="toc-number">8.</span> <span class="toc-text">Training CNN from scratch on a small dataset</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Dataset"><span class="toc-number">9.</span> <span class="toc-text">Dataset</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Extracting-zip-files"><span class="toc-number">10.</span> <span class="toc-text">Extracting zip files</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Importing-Dependencies"><span class="toc-number">11.</span> <span class="toc-text">Importing Dependencies</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Preparing-Data"><span class="toc-number">12.</span> <span class="toc-text">Preparing Data</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Train-Validation-Split"><span class="toc-number">13.</span> <span class="toc-text">Train Validation Split</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Building-the-CNN"><span class="toc-number">14.</span> <span class="toc-text">Building the CNN</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Data-preprocessing"><span class="toc-number">15.</span> <span class="toc-text">Data preprocessing</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Train-Fit"><span class="toc-number">16.</span> <span class="toc-text">Train &#x2F; Fit</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Predict"><span class="toc-number">17.</span> <span class="toc-text">Predict</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Creating-Submission-File"><span class="toc-number">18.</span> <span class="toc-text">Creating Submission File</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Data-Augmentation"><span class="toc-number">19.</span> <span class="toc-text">Data Augmentation</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Note"><span class="toc-number">19.0.1.</span> <span class="toc-text">Note</span></a></li></ol></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://zarif98sjs.github.io/mindcraft/CNN-Part-1/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-1/&text=CNN : Watching the world through Neural Networks - Part 1"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-1/&title=CNN : Watching the world through Neural Networks - Part 1"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-1/&is_video=false&description=CNN : Watching the world through Neural Networks - Part 1"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=CNN : Watching the world through Neural Networks - Part 1&body=Check out this article: https://zarif98sjs.github.io/mindcraft/CNN-Part-1/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-1/&title=CNN : Watching the world through Neural Networks - Part 1"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-1/&title=CNN : Watching the world through Neural Networks - Part 1"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-1/&title=CNN : Watching the world through Neural Networks - Part 1"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-1/&title=CNN : Watching the world through Neural Networks - Part 1"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-1/&name=CNN : Watching the world through Neural Networks - Part 1&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://zarif98sjs.github.io/mindcraft/CNN-Part-1/&t=CNN : Watching the world through Neural Networks - Part 1"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2019-2021
    Md. Zarif Ul Alam
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/mindcraft/">Home</a></li>
         
          <li><a href="/mindcraft/archives/">Archive</a></li>
         
          <li><a href="https://zarif98sjs.github.io/">About Me</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->
 
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script> 




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script> 
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/mindcraft/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Umami Analytics -->

<!-- Disqus Comments -->


</body>
</html>
