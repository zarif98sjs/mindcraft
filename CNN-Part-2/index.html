<!DOCTYPE html>
<html lang=en>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=5" />
    <meta name="description" content="Using a pretrained convnetA common and highly effective approach to deep learning on small image datasets is to use a pretrained network. A pretrained network is a saved network that was previously tr">
<meta property="og:type" content="article">
<meta property="og:title" content="CNN : Watching the world through Neural Networks - Part 2">
<meta property="og:url" content="https://zarif98sjs.github.io/mindcraft/CNN-Part-2/index.html">
<meta property="og:site_name" content="MindCraft">
<meta property="og:description" content="Using a pretrained convnetA common and highly effective approach to deep learning on small image datasets is to use a pretrained network. A pretrained network is a saved network that was previously tr">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://zarif98sjs.github.io/cnn_p2_img.png">
<meta property="og:image" content="https://zarif98sjs.github.io/mindcraft/CNN-Part-2/output_36_0.png">
<meta property="og:image" content="https://zarif98sjs.github.io/mindcraft/CNN-Part-2/output_36_1.png">
<meta property="article:published_time" content="2020-06-30T00:00:00.000Z">
<meta property="article:modified_time" content="2021-04-27T07:33:58.157Z">
<meta property="article:author" content="Md. Zarif Ul Alam">
<meta property="article:tag" content="Algo, DS, Software &amp; What Not!">
<meta property="article:tag" content="Learning Notes">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://zarif98sjs.github.io/cnn_p2_img.png">
    
    
      
        
          <link rel="shortcut icon" href="/mindcraft/images/favicon.ico">
        
      
      
        
          <link rel="icon" type="image/png" href="/mindcraft/images/favicon-192x192.png" sizes="192x192">
        
      
      
        
          <link rel="apple-touch-icon" sizes="180x180" href="/mindcraft/images/apple-touch-icon.png">
        
      
    
    <!-- title -->
    <title>CNN : Watching the world through Neural Networks - Part 2</title>
    <!-- styles -->
    
<link rel="stylesheet" href="/mindcraft/css/style.css">

    <!-- persian styles -->
    
    <!-- rss -->
    
    
	<!-- mathjax -->
	
		<script type="text/x-mathjax-config">
		  MathJax.Hub.Config({
			tex2jax: {
			  skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'],
			  inlineMath: [['$','$']]
			}
		  });
		</script>
		<script src='https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-MML-AM_CHTML' async></script>
	
<meta name="generator" content="Hexo 5.4.0"></head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#" aria-label="Menu"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" "Top" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/mindcraft/">Home</a></li>
         
          <li><a href="/mindcraft/archives/">Archive</a></li>
         
          <li><a href="https://zarif98sjs.github.io/">About Me</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" aria-label="Previous post " href="/mindcraft/CNN-Part-3/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" aria-label="Next post " href="/mindcraft/CNN-Part-1/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" aria-label="Back to top " href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" aria-label="Share post " href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://zarif98sjs.github.io/mindcraft/CNN-Part-2/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-2/&text=CNN : Watching the world through Neural Networks - Part 2"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-2/&title=CNN : Watching the world through Neural Networks - Part 2"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-2/&is_video=false&description=CNN : Watching the world through Neural Networks - Part 2"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=CNN : Watching the world through Neural Networks - Part 2&body=Check out this article: https://zarif98sjs.github.io/mindcraft/CNN-Part-2/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-2/&title=CNN : Watching the world through Neural Networks - Part 2"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-2/&title=CNN : Watching the world through Neural Networks - Part 2"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-2/&title=CNN : Watching the world through Neural Networks - Part 2"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-2/&title=CNN : Watching the world through Neural Networks - Part 2"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-2/&name=CNN : Watching the world through Neural Networks - Part 2&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://zarif98sjs.github.io/mindcraft/CNN-Part-2/&t=CNN : Watching the world through Neural Networks - Part 2"><i class="fab fa-hacker-news " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Using-a-pretrained-convnet"><span class="toc-number">1.</span> <span class="toc-text">Using a pretrained convnet</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Imagenet-Dataset-amp-VGG-16"><span class="toc-number">2.</span> <span class="toc-text">Imagenet Dataset &amp; VGG 16</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Procedure"><span class="toc-number">3.</span> <span class="toc-text">Procedure</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Method-1-Feature-Extraction"><span class="toc-number">4.</span> <span class="toc-text">Method #1 : Feature Extraction</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Old-Preprocessings"><span class="toc-number">4.1.</span> <span class="toc-text">Old Preprocessings</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#VGG-16"><span class="toc-number">4.2.</span> <span class="toc-text">VGG-16</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Architecture"><span class="toc-number">4.3.</span> <span class="toc-text">Architecture</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#What-to-do-now"><span class="toc-number">4.4.</span> <span class="toc-text">What to do now ?</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Method-1-Part-1-Fast-Feature-Extraction-without-Data-Augmentation"><span class="toc-number">5.</span> <span class="toc-text">Method 1 Part 1 : Fast Feature Extraction without Data Augmentation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Prediction"><span class="toc-number">5.1.</span> <span class="toc-text">Prediction</span></a></li></ol></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index py4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        CNN : Watching the world through Neural Networks - Part 2
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">Md. Zarif Ul Alam</span>
      </span>
      
    <div class="postdate">
      
        <time datetime="2020-06-30T00:00:00.000Z" itemprop="datePublished">2020-06-30</time>
        
      
    </div>


      
    <div class="article-category">
        <i class="fas fa-archive"></i>
        <a class="category-link" href="/mindcraft/categories/Machine-Learning/">Machine Learning</a> › <a class="category-link" href="/mindcraft/categories/Machine-Learning/Tensorflow/">Tensorflow</a>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link-link" href="/mindcraft/tags/Algo-DS-Software-What-Not/" rel="tag">Algo, DS, Software & What Not!</a>, <a class="tag-link-link" href="/mindcraft/tags/Learning-Notes/" rel="tag">Learning Notes</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <h1 id="Using-a-pretrained-convnet"><a href="#Using-a-pretrained-convnet" class="headerlink" title="Using a pretrained convnet"></a>Using a pretrained convnet</h1><p>A common and highly effective approach to deep learning on small image datasets is to use a pretrained network. A pretrained network is a saved network that was previously trained on a large dataset, typically on a large-scale image-classification task. If this original dataset is large enough and general enough, then the spatial hierarchy of features learned by the pretrained network can effectively act as a generic model of the visual world</p>
<p>For instance, you might train a network on ImageNet (where classes are mostly animals and everyday objects) and then repurpose this trained network for something as remote as identifying furniture items in images. Such portability of learned features across different problems is a key advantage of deep learning compared to many older, shallow-learning approaches, and it makes deep learning very effective for small-data problems</p>
<h1 id="Imagenet-Dataset-amp-VGG-16"><a href="#Imagenet-Dataset-amp-VGG-16" class="headerlink" title="Imagenet Dataset &amp; VGG 16"></a>Imagenet Dataset &amp; VGG 16</h1><p><strong>In this case, let’s consider a large convnet trained on the ImageNet dataset (1.4 million labeled images and 1,000 different classes). ImageNet contains many animal classes, including different species of cats and dogs, and you can thus expect to perform well on the dogs-versus-cats classification problem . You’ll use the VGG16 architecture, developed by Karen Simonyan and Andrew Zisserman in 2014; it’s a simple and widely used convnet architecture for ImageNet. its architecture is similar to<br>what you’re already familiar with and is easy to understand without introducing any new concepts</strong></p>
<h1 id="Procedure"><a href="#Procedure" class="headerlink" title="Procedure"></a>Procedure</h1><p>There are two ways to use a pretrained network: </p>
<ul>
<li>Feature Extraction</li>
<li>Fine Tuning</li>
</ul>
<h1 id="Method-1-Feature-Extraction"><a href="#Method-1-Feature-Extraction" class="headerlink" title="Method #1 : Feature Extraction"></a>Method #1 : Feature Extraction</h1><p>Feature extraction consists of using the representations learned by a previous network to extract interesting features from new samples. These features are then run through a new classifier, which is trained from scratch</p>
<p>Feature extraction consists of taking the convolutional base of a previously trained network, running the new data through it, and training a new classifier on top of the output</p>
<p><img src="/cnn_p2_img.png" alt="image.png"></p>
<p><strong>Why only reuse the convolutional base? Could you reuse the densely connected classifier as well?</strong></p>
<ul>
<li>In general, doing so should be <em>avoided</em></li>
<li>The reason is that the representations learned by the convolutional base are likely to be more <em>generic</em> and therefore more reusable</li>
<li>The feature maps of a convnet are presence maps of generic concepts over a picture, which is likely to be useful regardless of the computer-vision problem at hand</li>
<li>Representations found in densely connected layers no longer contain any information about <em>where objects are located</em> in the input image: these layers get rid of the <em>notion of space</em> , whereas the object location is still described by <em>convolutional feature maps</em></li>
<li>For problems where object location matters, densely connected features are largely useless</li>
</ul>
<p>Note that the level of generality (and therefore reusability) of the representations extracted by specific convolution layers depends on the depth of the layer in the model. Layers that come earlier in the model extract local, highly generic feature maps (such as visual edges, colors, and textures), whereas layers that are higher up extract more-abstract concepts (such as “cat ear” or “dog eye”)</p>
<p>So if your new dataset differs a lot from the dataset on which the original model was trained, you may be better off using only the first few layers of the model to do feature extraction, rather than using the entire convolutional base</p>
<h2 id="Old-Preprocessings"><a href="#Old-Preprocessings" class="headerlink" title="Old Preprocessings"></a>Old Preprocessings</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># This Python 3 environment comes with many helpful analytics libraries installed</span></span><br><span class="line"><span class="comment"># It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python</span></span><br><span class="line"><span class="comment"># For example, here&#x27;s several helpful packages to load</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np <span class="comment"># linear algebra</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd <span class="comment"># data processing, CSV file I/O (e.g. pd.read_csv)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Input data files are available in the read-only &quot;../input/&quot; directory</span></span><br><span class="line"><span class="comment"># For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">for</span> dirname, _, filenames <span class="keyword">in</span> os.walk(<span class="string">&#x27;/kaggle/input&#x27;</span>):</span><br><span class="line">    <span class="keyword">for</span> filename <span class="keyword">in</span> filenames:</span><br><span class="line">        <span class="built_in">print</span>(os.path.join(dirname, filename))</span><br><span class="line"></span><br><span class="line"><span class="comment"># You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using &quot;Save &amp; Run All&quot; </span></span><br><span class="line"><span class="comment"># You can also write temporary files to /kaggle/temp/, but they won&#x27;t be saved outside of the current session</span></span><br></pre></td></tr></table></figure>

<pre><code>/kaggle/input/dogs-vs-cats-redux-kernels-edition/sample_submission.csv
/kaggle/input/dogs-vs-cats-redux-kernels-edition/train.zip
/kaggle/input/dogs-vs-cats-redux-kernels-edition/test.zip
/kaggle/input/vgg16-mod/cats_and_dogs_transfer_learning_1_vgg16.h5
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">with</span> zipfile.ZipFile(<span class="string">&quot;../input/dogs-vs-cats-redux-kernels-edition/&quot;</span>+<span class="string">&quot;train&quot;</span>+<span class="string">&quot;.zip&quot;</span>,<span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> z:</span><br><span class="line">    z.extractall(<span class="string">&quot;.&quot;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> zipfile</span><br><span class="line"><span class="keyword">with</span> zipfile.ZipFile(<span class="string">&quot;../input/dogs-vs-cats-redux-kernels-edition/&quot;</span>+<span class="string">&quot;test&quot;</span>+<span class="string">&quot;.zip&quot;</span>,<span class="string">&quot;r&quot;</span>) <span class="keyword">as</span> z:</span><br><span class="line">    z.extractall(<span class="string">&quot;.&quot;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os, cv2, re, random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> img_to_array, load_img</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers, models, optimizers</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> K</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">img_width = <span class="number">150</span></span><br><span class="line">img_height = <span class="number">150</span></span><br><span class="line">TRAIN_DIR = <span class="string">&#x27;/kaggle/working/train/&#x27;</span></span><br><span class="line">TEST_DIR = <span class="string">&#x27;/kaggle/working/test/&#x27;</span></span><br><span class="line">train_images_dogs_cats = [TRAIN_DIR+i <span class="keyword">for</span> i <span class="keyword">in</span> os.listdir(TRAIN_DIR)] <span class="comment"># use this for full dataset</span></span><br><span class="line">test_images_dogs_cats = [TEST_DIR+i <span class="keyword">for</span> i <span class="keyword">in</span> os.listdir(TEST_DIR)]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(train_images_dogs_cats))</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(test_images_dogs_cats))</span><br></pre></td></tr></table></figure>

<pre><code>25000
12500
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">atoi</span>(<span class="params">text</span>):</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">int</span>(text) <span class="keyword">if</span> text.isdigit() <span class="keyword">else</span> text</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">natural_keys</span>(<span class="params">text</span>):</span></span><br><span class="line">    <span class="keyword">return</span> [ atoi(c) <span class="keyword">for</span> c <span class="keyword">in</span> re.split(<span class="string">&#x27;(\d+)&#x27;</span>, text) ]</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(natural_keys(<span class="string">&quot;cat.0.txt&quot;</span>))</span><br></pre></td></tr></table></figure>

<pre><code>[&#39;cat.&#39;, 0, &#39;.txt&#39;]
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">num_of_each_sample = <span class="number">1500</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_images_dogs_cats.sort(key=natural_keys)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(train_images_dogs_cats))</span><br></pre></td></tr></table></figure>

<pre><code>25000
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">train_images_dogs_cats = train_images_dogs_cats[<span class="number">0</span>:num_of_each_sample] + train_images_dogs_cats[<span class="number">12500</span>:<span class="number">12500</span>+num_of_each_sample] </span><br><span class="line">test_images_dogs_cats.sort(key=natural_keys)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(train_images_dogs_cats))</span><br></pre></td></tr></table></figure>

<pre><code>3000
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_data</span>(<span class="params">list_of_images</span>):</span></span><br><span class="line">    <span class="string">&quot;&quot;&quot;</span></span><br><span class="line"><span class="string">    Returns two arrays: </span></span><br><span class="line"><span class="string">        x is an array of resized images</span></span><br><span class="line"><span class="string">        y is an array of labels</span></span><br><span class="line"><span class="string">    &quot;&quot;&quot;</span></span><br><span class="line">    x = [] <span class="comment"># images as arrays</span></span><br><span class="line">    y = [] <span class="comment"># labels</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> image <span class="keyword">in</span> list_of_images:</span><br><span class="line">        x.append(cv2.resize(cv2.imread(image), (img_width,img_height), interpolation=cv2.INTER_CUBIC))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> list_of_images:</span><br><span class="line">        <span class="keyword">if</span> <span class="string">&#x27;dog&#x27;</span> <span class="keyword">in</span> i:</span><br><span class="line">            y.append(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">elif</span> <span class="string">&#x27;cat&#x27;</span> <span class="keyword">in</span> i:</span><br><span class="line">            y.append(<span class="number">0</span>)</span><br><span class="line">        <span class="comment">#else:</span></span><br><span class="line">            <span class="comment">#print(&#x27;neither cat nor dog name present in images&#x27;)</span></span><br><span class="line">    <span class="keyword">return</span> x, y</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">X, Y = prepare_data(train_images_dogs_cats)</span><br><span class="line"><span class="built_in">print</span>(K.image_data_format())</span><br></pre></td></tr></table></figure>

<pre><code>channels_last
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># First split the data in two sets, 80% for training, 20% for Val/Test)</span></span><br><span class="line">X_train, X_val, Y_train, Y_val = train_test_split(X,Y, test_size=<span class="number">0.333334</span>, random_state=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nb_train_samples = <span class="built_in">len</span>(X_train)</span><br><span class="line">nb_validation_samples = <span class="built_in">len</span>(X_val)</span><br><span class="line">batch_size = <span class="number">16</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(nb_train_samples)</span><br><span class="line"><span class="built_in">print</span>(nb_validation_samples)</span><br></pre></td></tr></table></figure>

<pre><code>1999
1001
</code></pre>
<h2 id="VGG-16"><a href="#VGG-16" class="headerlink" title="VGG-16"></a>VGG-16</h2><p><strong>The VGG16 model, among others, comes prepackaged with Keras. You can import it from the keras.applications module. Here’s the list of image-classification<br>models (all pretrained on the ImageNet dataset) that are available as part of <code>keras.applications</code> :</strong></p>
<ul>
<li>Xception</li>
<li>Inception V3</li>
<li>ResNet50</li>
<li>VGG16</li>
<li>VGG19</li>
<li>MobileNet</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras.applications <span class="keyword">import</span> VGG16</span><br><span class="line"></span><br><span class="line">conv_base = VGG16(weights=<span class="string">&#x27;imagenet&#x27;</span>,include_top=<span class="literal">False</span>,input_shape=(img_width, img_height, <span class="number">3</span>))</span><br></pre></td></tr></table></figure>

<pre><code>Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5
58892288/58889256 [==============================] - 1s 0us/step
</code></pre>
<p>You pass three arguments to the constructor :</p>
<ul>
<li><code>weights</code> specifies the weight checkpoint from which to initialize the model</li>
<li><code>include_top</code> refers to including (or not) the densely connected classifier on top of the network</li>
<li><code>input_shape</code> is the shape of the image tensors that you’ll feed to the network (This argument is purely optional: if you don’t pass it, the network will be able to process inputs of any size)</li>
</ul>
<h2 id="Architecture"><a href="#Architecture" class="headerlink" title="Architecture"></a>Architecture</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conv_base.summary()</span><br></pre></td></tr></table></figure>

<pre><code>Model: &quot;vgg16&quot;
_________________________________________________________________
Layer (type)                 Output Shape              Param #   
=================================================================
input_1 (InputLayer)         [(None, 150, 150, 3)]     0         
_________________________________________________________________
block1_conv1 (Conv2D)        (None, 150, 150, 64)      1792      
_________________________________________________________________
block1_conv2 (Conv2D)        (None, 150, 150, 64)      36928     
_________________________________________________________________
block1_pool (MaxPooling2D)   (None, 75, 75, 64)        0         
_________________________________________________________________
block2_conv1 (Conv2D)        (None, 75, 75, 128)       73856     
_________________________________________________________________
block2_conv2 (Conv2D)        (None, 75, 75, 128)       147584    
_________________________________________________________________
block2_pool (MaxPooling2D)   (None, 37, 37, 128)       0         
_________________________________________________________________
block3_conv1 (Conv2D)        (None, 37, 37, 256)       295168    
_________________________________________________________________
block3_conv2 (Conv2D)        (None, 37, 37, 256)       590080    
_________________________________________________________________
block3_conv3 (Conv2D)        (None, 37, 37, 256)       590080    
_________________________________________________________________
block3_pool (MaxPooling2D)   (None, 18, 18, 256)       0         
_________________________________________________________________
block4_conv1 (Conv2D)        (None, 18, 18, 512)       1180160   
_________________________________________________________________
block4_conv2 (Conv2D)        (None, 18, 18, 512)       2359808   
_________________________________________________________________
block4_conv3 (Conv2D)        (None, 18, 18, 512)       2359808   
_________________________________________________________________
block4_pool (MaxPooling2D)   (None, 9, 9, 512)         0         
_________________________________________________________________
block5_conv1 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_conv2 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_conv3 (Conv2D)        (None, 9, 9, 512)         2359808   
_________________________________________________________________
block5_pool (MaxPooling2D)   (None, 4, 4, 512)         0         
=================================================================
Total params: 14,714,688
Trainable params: 14,714,688
Non-trainable params: 0
_________________________________________________________________
</code></pre>
<p><strong>The final feature map has shape <code>(4, 4, 512)</code>. That’s the feature on top of which you’ll stick a densely connected classifier.</strong></p>
<h2 id="What-to-do-now"><a href="#What-to-do-now" class="headerlink" title="What to do now ?"></a>What to do now ?</h2><p><strong>There are 2 possible options :</strong></p>
<ul>
<li><p>Running the convolutional base over your dataset, recording its output to a Numpy array on disk, and then using this data as input to a standalone, <em>densely connected classifier</em></p>
<ul>
<li>Merits : fast </li>
<li>Demerits : can’t use data augmentation</li>
</ul>
</li>
</ul>
<ul>
<li><p>Extending the model you have <code>(conv_base)</code> by adding Dense layers on top, and running the whole thing <em>end to end</em> on the input data. </p>
<ul>
<li>Merits : allows data augmentation</li>
<li>Demerits : slow and expensive </li>
</ul>
</li>
</ul>
<h1 id="Method-1-Part-1-Fast-Feature-Extraction-without-Data-Augmentation"><a href="#Method-1-Part-1-Fast-Feature-Extraction-without-Data-Augmentation" class="headerlink" title="Method 1 Part 1 : Fast Feature Extraction without Data Augmentation"></a>Method 1 Part 1 : Fast Feature Extraction without Data Augmentation</h1><p><strong>Features are extracted by the <code>predict</code> method of the <code>conv_base</code> model</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">datagen = ImageDataGenerator(rescale=<span class="number">1.</span>/<span class="number">255</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_features</span>(<span class="params">X_INPUT,Y_OUTPUT, sample_count</span>):</span></span><br><span class="line">    features = np.zeros(shape=(sample_count, <span class="number">4</span>, <span class="number">4</span>, <span class="number">512</span>))</span><br><span class="line">    labels = np.zeros(shape=(sample_count))</span><br><span class="line">    generator = datagen.flow(np.array(X_INPUT),Y_OUTPUT,batch_size=batch_size)</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> inputs_batch, labels_batch <span class="keyword">in</span> generator:</span><br><span class="line">        features_batch = conv_base.predict(inputs_batch)</span><br><span class="line">        features[i * batch_size : (i + <span class="number">1</span>) * batch_size] = features_batch</span><br><span class="line">        labels[i * batch_size : (i + <span class="number">1</span>) * batch_size] = labels_batch</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> i * batch_size &gt;= sample_count:</span><br><span class="line">            <span class="comment"># Note that since generators yield data indefinitely in a loop,</span></span><br><span class="line">            <span class="comment"># we must `break` after every image has been seen once.</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> features, labels</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_features, train_labels = extract_features(X_train,Y_train, nb_train_samples)</span><br><span class="line">validation_features, validation_labels = extract_features(X_val, Y_val,nb_validation_samples)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">train_features = np.reshape(train_features, (nb_train_samples, <span class="number">4</span> * <span class="number">4</span> * <span class="number">512</span>))</span><br><span class="line">validation_features = np.reshape(validation_features, (nb_validation_samples, <span class="number">4</span> * <span class="number">4</span> * <span class="number">512</span>))</span><br></pre></td></tr></table></figure>

<p><strong>dropout is used for regularization</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> models</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> layers</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers</span><br><span class="line"></span><br><span class="line">model = models.Sequential()</span><br><span class="line">model.add(layers.Dense(<span class="number">256</span>, activation=<span class="string">&#x27;relu&#x27;</span>, input_dim=<span class="number">4</span> * <span class="number">4</span> * <span class="number">512</span>))</span><br><span class="line">model.add(layers.Dropout(<span class="number">0.5</span>))</span><br><span class="line">model.add(layers.Dense(<span class="number">1</span>, activation=<span class="string">&#x27;sigmoid&#x27;</span>))</span><br><span class="line"></span><br><span class="line">model.<span class="built_in">compile</span>(optimizer=optimizers.RMSprop(lr=<span class="number">2e-5</span>),</span><br><span class="line">              loss=<span class="string">&#x27;binary_crossentropy&#x27;</span>,</span><br><span class="line">              metrics=[<span class="string">&#x27;acc&#x27;</span>])</span><br><span class="line"></span><br><span class="line">history = model.fit(train_features, train_labels,</span><br><span class="line">                    epochs=<span class="number">30</span>,</span><br><span class="line">                    batch_size=batch_size,</span><br><span class="line">                    validation_data=(validation_features, validation_labels))</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 1/30
125/125 [==============================] - 2s 14ms/step - loss: 0.5894 - acc: 0.6768 - val_loss: 0.3824 - val_acc: 0.8511
Epoch 2/30
125/125 [==============================] - 2s 12ms/step - loss: 0.3750 - acc: 0.8369 - val_loss: 0.3344 - val_acc: 0.8472
Epoch 3/30
125/125 [==============================] - 2s 13ms/step - loss: 0.3237 - acc: 0.8609 - val_loss: 0.2774 - val_acc: 0.8881
Epoch 4/30
125/125 [==============================] - 2s 12ms/step - loss: 0.2762 - acc: 0.8834 - val_loss: 0.2614 - val_acc: 0.8911
Epoch 5/30
125/125 [==============================] - 2s 16ms/step - loss: 0.2553 - acc: 0.8909 - val_loss: 0.2571 - val_acc: 0.8991
Epoch 6/30
125/125 [==============================] - 2s 12ms/step - loss: 0.2368 - acc: 0.9015 - val_loss: 0.2406 - val_acc: 0.9021
Epoch 7/30
125/125 [==============================] - 2s 13ms/step - loss: 0.2095 - acc: 0.9185 - val_loss: 0.2362 - val_acc: 0.9001
Epoch 8/30
125/125 [==============================] - 2s 13ms/step - loss: 0.2087 - acc: 0.9105 - val_loss: 0.2440 - val_acc: 0.9001
Epoch 9/30
125/125 [==============================] - 2s 13ms/step - loss: 0.1881 - acc: 0.9255 - val_loss: 0.2300 - val_acc: 0.9011
Epoch 10/30
125/125 [==============================] - 2s 13ms/step - loss: 0.1772 - acc: 0.9325 - val_loss: 0.2271 - val_acc: 0.9041
Epoch 11/30
125/125 [==============================] - 2s 13ms/step - loss: 0.1727 - acc: 0.9350 - val_loss: 0.2278 - val_acc: 0.9011
Epoch 12/30
125/125 [==============================] - 2s 13ms/step - loss: 0.1598 - acc: 0.9365 - val_loss: 0.2252 - val_acc: 0.9041
Epoch 13/30
125/125 [==============================] - 1s 12ms/step - loss: 0.1529 - acc: 0.9400 - val_loss: 0.2246 - val_acc: 0.9011
Epoch 14/30
125/125 [==============================] - 1s 12ms/step - loss: 0.1449 - acc: 0.9470 - val_loss: 0.2283 - val_acc: 0.9031
Epoch 15/30
125/125 [==============================] - 1s 12ms/step - loss: 0.1403 - acc: 0.9465 - val_loss: 0.2240 - val_acc: 0.9061
Epoch 16/30
125/125 [==============================] - 2s 12ms/step - loss: 0.1299 - acc: 0.9505 - val_loss: 0.2246 - val_acc: 0.9051
Epoch 17/30
125/125 [==============================] - 1s 12ms/step - loss: 0.1247 - acc: 0.9550 - val_loss: 0.2233 - val_acc: 0.9011
Epoch 18/30
125/125 [==============================] - 1s 12ms/step - loss: 0.1173 - acc: 0.9565 - val_loss: 0.2295 - val_acc: 0.9031
Epoch 19/30
125/125 [==============================] - 1s 12ms/step - loss: 0.1187 - acc: 0.9580 - val_loss: 0.2240 - val_acc: 0.9021
Epoch 20/30
125/125 [==============================] - 1s 12ms/step - loss: 0.1052 - acc: 0.9695 - val_loss: 0.2264 - val_acc: 0.9041
Epoch 21/30
125/125 [==============================] - 1s 12ms/step - loss: 0.1025 - acc: 0.9680 - val_loss: 0.2255 - val_acc: 0.9081
Epoch 22/30
125/125 [==============================] - 1s 12ms/step - loss: 0.1045 - acc: 0.9665 - val_loss: 0.2268 - val_acc: 0.9051
Epoch 23/30
125/125 [==============================] - 1s 12ms/step - loss: 0.0942 - acc: 0.9670 - val_loss: 0.2281 - val_acc: 0.9051
Epoch 24/30
125/125 [==============================] - 2s 12ms/step - loss: 0.0901 - acc: 0.9725 - val_loss: 0.2306 - val_acc: 0.9051
Epoch 25/30
125/125 [==============================] - 2s 12ms/step - loss: 0.0859 - acc: 0.9750 - val_loss: 0.2349 - val_acc: 0.9031
Epoch 26/30
125/125 [==============================] - 2s 12ms/step - loss: 0.0851 - acc: 0.9700 - val_loss: 0.2308 - val_acc: 0.9041
Epoch 27/30
125/125 [==============================] - 1s 12ms/step - loss: 0.0824 - acc: 0.9725 - val_loss: 0.2319 - val_acc: 0.9031
Epoch 28/30
125/125 [==============================] - 1s 12ms/step - loss: 0.0796 - acc: 0.9760 - val_loss: 0.2337 - val_acc: 0.9031
Epoch 29/30
125/125 [==============================] - 1s 12ms/step - loss: 0.0761 - acc: 0.9785 - val_loss: 0.2350 - val_acc: 0.9041
Epoch 30/30
125/125 [==============================] - 1s 12ms/step - loss: 0.0720 - acc: 0.9815 - val_loss: 0.2449 - val_acc: 0.9081
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.save(<span class="string">&#x27;dogsVScats_TL_VGG16_feature_extraction.h5&#x27;</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">acc = history.history[<span class="string">&#x27;acc&#x27;</span>]</span><br><span class="line">val_acc = history.history[<span class="string">&#x27;val_acc&#x27;</span>]</span><br><span class="line">loss = history.history[<span class="string">&#x27;loss&#x27;</span>]</span><br><span class="line">val_loss = history.history[<span class="string">&#x27;val_loss&#x27;</span>]</span><br><span class="line"></span><br><span class="line">epochs = <span class="built_in">range</span>(<span class="built_in">len</span>(acc))</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, acc, <span class="string">&#x27;bo&#x27;</span>, label=<span class="string">&#x27;Training acc&#x27;</span>)</span><br><span class="line">plt.plot(epochs, val_acc, <span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;Validation acc&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and validation accuracy&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.figure()</span><br><span class="line"></span><br><span class="line">plt.plot(epochs, loss, <span class="string">&#x27;bo&#x27;</span>, label=<span class="string">&#x27;Training loss&#x27;</span>)</span><br><span class="line">plt.plot(epochs, val_loss, <span class="string">&#x27;b&#x27;</span>, label=<span class="string">&#x27;Validation loss&#x27;</span>)</span><br><span class="line">plt.title(<span class="string">&#x27;Training and validation loss&#x27;</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>


<p><img src="output_36_0.png" alt="png"></p>
<p><img src="output_36_1.png" alt="png"></p>
<p>Validation accuracy has increased to 90 % . Much better than the previous techniques . But the plots indicate that the model is overfitting despite using droput with a fairly large rate . The reason is that  this technique doesn’t use data augmentation, which is essential for preventing overfitting with small image datasets</p>
<h2 id="Prediction"><a href="#Prediction" class="headerlink" title="Prediction"></a>Prediction</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">X_test, Y_test = prepare_data(test_images_dogs_cats) <span class="comment">#Y_test in this case will be []</span></span><br><span class="line">nb_test_samples = <span class="built_in">len</span>(test_images_dogs_cats)</span><br><span class="line"><span class="built_in">print</span>(nb_test_samples)</span><br></pre></td></tr></table></figure>

<pre><code>12500
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">extract_test_features</span>(<span class="params">X_INPUT, sample_count</span>):</span></span><br><span class="line">    features = np.zeros(shape=(sample_count, <span class="number">4</span>, <span class="number">4</span>, <span class="number">512</span>))</span><br><span class="line">    generator = datagen.flow(np.array(X_INPUT),batch_size=batch_size)</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> inputs_batch <span class="keyword">in</span> generator:</span><br><span class="line">        features_batch = conv_base.predict(inputs_batch)</span><br><span class="line">        features[i * batch_size : (i + <span class="number">1</span>) * batch_size] = features_batch</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> i * batch_size &gt;= sample_count:</span><br><span class="line">            <span class="comment"># Note that since generators yield data indefinitely in a loop,</span></span><br><span class="line">            <span class="comment"># we must `break` after every image has been seen once.</span></span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> features</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test_features = extract_test_features(X_test,nb_test_samples)</span><br><span class="line">test_features = np.reshape(test_features, (nb_test_samples, <span class="number">4</span> * <span class="number">4</span> * <span class="number">512</span>))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">prediction_probabilities = model.predict(test_features, verbose=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<pre><code>391/391 [==============================] - 1s 3ms/step
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># prediction_probabilities_binary = []</span></span><br><span class="line"><span class="comment"># for p in prediction_probabilities :</span></span><br><span class="line"><span class="comment">#     if p &gt;= 0.5 :</span></span><br><span class="line"><span class="comment">#         prediction_probabilities_binary.append(1)</span></span><br><span class="line"><span class="comment">#     else:</span></span><br><span class="line"><span class="comment">#         prediction_probabilities_binary.append(0)</span></span><br><span class="line"><span class="comment"># print(len(prediction_probabilities_binary))</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(prediction_probabilities.shape)</span><br></pre></td></tr></table></figure>

<pre><code>(12500, 1)
</code></pre>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">counter = <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(test_images_dogs_cats) + <span class="number">1</span>)</span><br><span class="line">solution = pd.DataFrame(&#123;<span class="string">&quot;id&quot;</span>: counter, <span class="string">&quot;label&quot;</span>:<span class="built_in">list</span>(prediction_probabilities)&#125;)</span><br><span class="line">cols = [<span class="string">&#x27;label&#x27;</span>]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> cols:</span><br><span class="line">    solution[col] = solution[col].<span class="built_in">map</span>(<span class="keyword">lambda</span> x: <span class="built_in">str</span>(x).lstrip(<span class="string">&#x27;[&#x27;</span>).rstrip(<span class="string">&#x27;]&#x27;</span>)).astype(<span class="built_in">float</span>)</span><br><span class="line"></span><br><span class="line">solution.to_csv(<span class="string">&quot;dogsVScats_TL_VGG16_feature_extraction.csv&quot;</span>, index = <span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

  </div>
</article>



        
          <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/mindcraft/">Home</a></li>
         
          <li><a href="/mindcraft/archives/">Archive</a></li>
         
          <li><a href="https://zarif98sjs.github.io/">About Me</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#Using-a-pretrained-convnet"><span class="toc-number">1.</span> <span class="toc-text">Using a pretrained convnet</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Imagenet-Dataset-amp-VGG-16"><span class="toc-number">2.</span> <span class="toc-text">Imagenet Dataset &amp; VGG 16</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Procedure"><span class="toc-number">3.</span> <span class="toc-text">Procedure</span></a></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Method-1-Feature-Extraction"><span class="toc-number">4.</span> <span class="toc-text">Method #1 : Feature Extraction</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Old-Preprocessings"><span class="toc-number">4.1.</span> <span class="toc-text">Old Preprocessings</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#VGG-16"><span class="toc-number">4.2.</span> <span class="toc-text">VGG-16</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Architecture"><span class="toc-number">4.3.</span> <span class="toc-text">Architecture</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#What-to-do-now"><span class="toc-number">4.4.</span> <span class="toc-text">What to do now ?</span></a></li></ol></li><li class="toc-item toc-level-1"><a class="toc-link" href="#Method-1-Part-1-Fast-Feature-Extraction-without-Data-Augmentation"><span class="toc-number">5.</span> <span class="toc-text">Method 1 Part 1 : Fast Feature Extraction without Data Augmentation</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#Prediction"><span class="toc-number">5.1.</span> <span class="toc-text">Prediction</span></a></li></ol></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.facebook.com/sharer.php?u=https://zarif98sjs.github.io/mindcraft/CNN-Part-2/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://twitter.com/share?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-2/&text=CNN : Watching the world through Neural Networks - Part 2"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.linkedin.com/shareArticle?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-2/&title=CNN : Watching the world through Neural Networks - Part 2"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://pinterest.com/pin/create/bookmarklet/?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-2/&is_video=false&description=CNN : Watching the world through Neural Networks - Part 2"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=CNN : Watching the world through Neural Networks - Part 2&body=Check out this article: https://zarif98sjs.github.io/mindcraft/CNN-Part-2/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://getpocket.com/save?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-2/&title=CNN : Watching the world through Neural Networks - Part 2"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://reddit.com/submit?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-2/&title=CNN : Watching the world through Neural Networks - Part 2"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.stumbleupon.com/submit?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-2/&title=CNN : Watching the world through Neural Networks - Part 2"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://digg.com/submit?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-2/&title=CNN : Watching the world through Neural Networks - Part 2"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="http://www.tumblr.com/share/link?url=https://zarif98sjs.github.io/mindcraft/CNN-Part-2/&name=CNN : Watching the world through Neural Networks - Part 2&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" target="_blank" rel="noopener" href="https://news.ycombinator.com/submitlink?u=https://zarif98sjs.github.io/mindcraft/CNN-Part-2/&t=CNN : Watching the world through Neural Networks - Part 2"><i class="fab fa-hacker-news fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

        
        <footer id="footer">
  <div class="footer-left">
    Copyright &copy;
    
    
    2019-2021
    Md. Zarif Ul Alam
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/mindcraft/">Home</a></li>
         
          <li><a href="/mindcraft/archives/">Archive</a></li>
         
          <li><a href="https://zarif98sjs.github.io/">About Me</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

    </div>
    <!-- styles -->



  <link rel="preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.2/css/all.min.css" crossorigin="anonymous" onload="this.onload=null;this.rel='stylesheet'"/>


    <!-- jquery -->
 
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.6.0/jquery.min.js" crossorigin="anonymous"></script> 




<!-- clipboard -->

  
    <script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.7/clipboard.min.js" crossorigin="anonymous"></script> 
  
  <script type="text/javascript">
  $(function() {
    // copy-btn HTML
    var btn = "<span class=\"btn-copy tooltipped tooltipped-sw\" aria-label=\"Copy to clipboard!\">";
    btn += '<i class="far fa-clone"></i>';
    btn += '</span>'; 
    // mount it!
    $(".highlight table").before(btn);
    var clip = new ClipboardJS('.btn-copy', {
      text: function(trigger) {
        return Array.from(trigger.nextElementSibling.querySelectorAll('.code')).reduce((str,it)=>str+it.innerText+'\n','')
      }
    });
    clip.on('success', function(e) {
      e.trigger.setAttribute('aria-label', "Copied!");
      e.clearSelection();
    })
  })
  </script>


<script src="/mindcraft/js/main.js"></script>

<!-- search -->

<!-- Google Analytics -->

<!-- Baidu Analytics -->

<!-- Cloudflare Analytics -->

<!-- Umami Analytics -->

<!-- Disqus Comments -->


</body>
</html>
